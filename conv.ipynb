{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: The Learning Problem\n",
    "\n",
    "We have a large dataset of $(x, y)$ pairs where $x$ denotes a vector of features and $y$ denotes the label for that feature vector. We want to learn a function $h(x)$ that maps features to labels, with good generalization accuracy. We do this by minimizing a loss function computed on our dataset: $ \\sum_{i=1}^{N} L(y_i, h(x_i)) $. There are many loss functions we can choose. We have gone over the cross-entropy loss and variants of the squared error loss functions in previous workshops, and we will once again consider those today. \n",
    "\n",
    "\n",
    "### Review: A Single \"Neuron\", aka the Perceptron\n",
    "\n",
    "![perceptron](https://raw.githubusercontent.com/rohan-varma/rohan-blog/gh-pages/images/perceptron.png)\n",
    "\n",
    "A single perceptron first calculates a **weighted sum** of our inputs. This means that we multiply each of our features $(x_1, x_2, ... x_n) \\in x$ with an associated weight $(w_1, w_2, ... w_n)$ . We then take the sign of this linear combination, which and the sign tells us whether to classify this instance as a positive or negative example.\n",
    "\n",
    "$h(x) = sign(w^Tx + b) $\n",
    "\n",
    "We then moved on to logistic regression, where we changed our sign function to instead be a sigmoid ($\\sigma$) function. As a reminder, here's the sigmoid function: \n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)\n",
    "\n",
    "Therefore, the function we compute for logistic regression is $h(x) = \\sigma (w^Tx + b)$. \n",
    "\n",
    "The sigmoid function is commonly referred to as an \"activation\" function. When we say that a \"neuron computes an activation function\", it means that a standard linear combination is calculated ($w^Tx + b$) and then we apply a _non linear_ function to it, such as the sigmoid function. \n",
    "\n",
    "Here are a few other common activation functions: \n",
    "\n",
    "![tanh](http://www.dplot.com/functions/tanh.png)\n",
    "![relu](https://i.stack.imgur.com/8CGlM.png)\n",
    "\n",
    "\n",
    "### Review: From binary to multi-class classification\n",
    "\n",
    "The most important change in moving from a binary (negative/positive) classification model to one that can classify training instances into many different classes (say, 10, for MNIST) is that our vector of weights $w$ changes into a matrix $W$. \n",
    "\n",
    "Each row of weights we learn represents the parameters for a certain class: \n",
    "\n",
    "![weights](https://raw.githubusercontent.com/rohan-varma/rohan-blog/gh-pages/images/imagemap.jpg)\n",
    "\n",
    "We also want to take our output and normalize the results so that they all sum to one, so that we can interpret them as probabilities. This is commonly done using the _softmax_ function, which takes in a vector and returns another vector who's elements sum to 1, and each element is proportional in scale to what it was in the original vector. In binary classification we used the sigmoid function to compute probabilities. Now since we have a vector, we use the softmax function.  \n",
    "\n",
    "Here is our current model of learning, then:\n",
    "\n",
    "$h(x) = softmax(Wx + b) $. \n",
    "\n",
    "### Building up the neural network\n",
    "\n",
    "Now that we've figured out how to linearly model multi-class classification, we can create a basic neural network. Consider what happens when we combine the idea of artificial neurons with our softmax classifier. Instead of computing a linear function  $Wx + b$ and immediately passing the output to a softmax function, we have an intermediate step: pass the output of our linear combination to a vector of artificial neurons, which each compute a nonlinear function. \n",
    "\n",
    "The output of this \"layer\" of neurons can be multiplied with a matrix of weights again, and we can apply our softmax function to this result to produce our predictions. \n",
    "\n",
    "** Original function **: $h(x) = softmax(Wx + b)$\n",
    "\n",
    "** Neural Network function **: $h(x) = softmax(W_2(nonlin(W_1x + b_1)) + b_2)$\n",
    "\n",
    "The key differences are that we have more biases and weights, as well as a larger composition of functions. This function is harder to optimize, and introduces a few interesting ideas about learning the weights with an algorithm known as backpropagation.\n",
    "\n",
    "This “intermediate step” is actually known as a hidden layer, and we have complete control over it, meaning that among other things, we can vary the number of parameters or connections between weights and neurons to obtain an optimal network. It’s also important to notice that we can stack an arbitrary amount of these hidden layers between the input and output of our network, and we can tune these layers individually. This lets us make our network as deep as we want it. For example, here’s what a neural network with two hidden layers would look like:\n",
    "\n",
    "![neuralnet](https://raw.githubusercontent.com/rohan-varma/rohan-blog/gh-pages/images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Example\n",
    "\n",
    "First, we'll run through a quick example on MNIST. We'll make sure that we can train a basic autoencoder, and see how it performs on reproducting MNIST image. Let's get started with some imports and helper functions to plot images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True) # reads in the MNIST dataset\n",
    "\n",
    "\n",
    "# a function that shows examples from the dataset. If num is specified (between 0 and 9), then only pictures with those labels will beused\n",
    "def show_pics(mnist, num = None):\n",
    "    to_show = list(range(10)) if not num else [num]*10 # figure out which numbers we should show\n",
    "    for i in range(100):\n",
    "        batch = mnist.train.next_batch(1) # gets some examples\n",
    "        pic, label = batch[0], batch[1]\n",
    "        if np.argmax(label) in to_show:\n",
    "            # use matplotlib to plot it\n",
    "            pic = pic.reshape((28,28))\n",
    "            plt.title(\"Label: {}\".format(np.argmax(label)))\n",
    "            plt.imshow(pic, cmap = 'binary')\n",
    "            plt.show()\n",
    "            to_show.remove(np.argmax(label))\n",
    "            \n",
    "# a function to plot the genned images\n",
    "def plot(samples, cur_epoch = None, shape = None):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28,28) if not shape else sample.reshape(shape), cmap='Greys_r')\n",
    "        # if epoch is not specified we just overwrite the existing image\n",
    "        plt.show()\n",
    "\n",
    "    return fig           \n",
    "\n",
    "#show_pics(mnist)\n",
    "show_pics(mnist, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "Below, we'll define our architecture, which will be a fully-connected network that is trained to produce its output. \n",
    "We'll also distinguish between transformations that encode images and transformations that take encodings and decode them back into images, and return them both. This is so that we can visualize what features were learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    \"\"\"Forwards x through the model\"\"\"\n",
    "    dense1 = tf.layers.dense(inputs=x, units=256, activation=tf.nn.relu)\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units = 100, activation=tf.nn.relu)\n",
    "    encoded_representation = dense2 \n",
    "    # that was the encoding, now let's do the decoding\n",
    "    dense3 = tf.layers.dense(inputs=dense2, units = 1024, activation = tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense3, units=784)\n",
    "    return logits, encoded_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our placeholder variables, loss function, and training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 784]) # output is itself\n",
    "# forward x through the model\n",
    "logits, encoder = model(x)\n",
    "# compute loss\n",
    "training_loss = tf.reduce_mean(tf.square(y-logits))\n",
    "optim_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll launch a session and run our model without any training for a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# launch a sess\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = mnist.train.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded_images = sess.run(logits, feed_dict = {x: x_batch})\n",
    "plot(decoded_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that seemed to work. Let's see what happens if we train the model for a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch generator for cifar10\n",
    "def my_generic_batch(dataset, n = 100):\n",
    "    # assumes dataset is a tuple of (x_train, y_train)\n",
    "    x_train, y_train = dataset\n",
    "    indices = np.random.randint(low = 0, high = x_train.shape[0], size = n)\n",
    "    x_batch, y_batch = x_train[indices], y_train[indices]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "#x_batch, y_batch = my_generic_batch((x_train, y_train))\n",
    "def train_model(optimizer, loss_func, model_logits = logits, dataset = \"mnist\", n_epochs = 1000, reshape = True):\n",
    "    # train the model a bit\n",
    "    for i in range(1000):\n",
    "        if dataset == \"mnist\":\n",
    "            x_batch, y_batch = mnist.train.next_batch(100)\n",
    "        else:\n",
    "            x_batch, y_batch = my_generic_batch(dataset)\n",
    "            if reshape:\n",
    "                x_batch = x_batch.reshape((100, 32 * 32 * 3))\n",
    "        optimizer.run(feed_dict = {x: x_batch, y: x_batch})\n",
    "        if i % 50 == 0:\n",
    "            cur_loss = loss_func.eval(feed_dict = {x: x_batch, y: x_batch})\n",
    "            print(cur_loss)\n",
    "            # sample an image from the model and plot it\n",
    "            decoded_images = sess.run(model_logits, feed_dict = {x: x_batch[:5]})\n",
    "            if dataset == \"mnist\":\n",
    "                plot(decoded_images)\n",
    "            else:\n",
    "                plot(decoded_images, shape = (32, 32, 3))\n",
    "\n",
    "#train_model(optim_step, training_loss)\n",
    "#train_model(optimizer = optim_step, loss_func = training_loss, model_logits = logits, dataset = \"mnist\", n_epochs = 1000)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample 10 test points\n",
    "test_images, _ = mnist.test.images[:10], mnist.test.labels[:10]\n",
    "# forward through the model\n",
    "decoded_images = sess.run(logits, feed_dict = {x: test_images})\n",
    "encoding = sess.run(encoder, feed_dict = {x: test_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(decoded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(encoding, shape = (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, our basic autoencoder seemed to work pretty well. Let's look at a few more advanced examples now. For this, we'll use the CIFAR10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cifar10_data = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = x_train[10:15]\n",
    "plot(samples, shape = (32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our old model again. It's redefined here for convenience (and because we had to change a few shapes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cifar_model(cifar_in):\n",
    "    \"\"\"Forwards x through the model\"\"\"\n",
    "    dense1 = tf.layers.dense(inputs=cifar_in, units=256, activation=tf.nn.relu)\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units = 100, activation=tf.nn.relu)\n",
    "    encoded_representation = dense2 \n",
    "    # that was the encoding, now let's do the decoding\n",
    "    dense3 = tf.layers.dense(inputs=dense2, units = 1024, activation = tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense3, units=32 * 32 * 3)\n",
    "    return logits, encoded_representation\n",
    "\n",
    "def better_cifar_model(cifar_in):\n",
    "    \"\"\"Forwards x through the model\"\"\"\n",
    "    # reshape input back into 32, 32, 3\n",
    "    x = tf.reshape(cifar_in, [-1, 32, 32, 3])\n",
    "    input_layer = x\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 32 * 10 * 64])\n",
    "    encoded_representation = pool2_flat\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=True)\n",
    "\n",
    "  # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=32 * 32 * 3)\n",
    "    return logits, encoded_representation\n",
    "    \n",
    "cifar_x = tf.placeholder(tf.float32, shape = [None, 32 * 32 * 3])\n",
    "cifar_y = tf.placeholder(tf.float32, shape = [None, 32 * 32 * 3]) # output is itself\n",
    "# forward x through the model\n",
    "model_type = \"good\"\n",
    "cifar_logits, cifar_encoder = cifar_model(cifar_x) if model_type == \"bad\" else better_cifar_model(cifar_x)\n",
    "# compute loss\n",
    "cifar_training_loss = tf.reduce_mean(tf.square(cifar_y-cifar_logits))\n",
    "cifar_optim_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cifar_training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tmp, y_tmp = x_train[:2].reshape((2, 3072)), x_train[:2].reshape((2, 3072))\n",
    "sess.run(cifar_optim_step, feed_dict = {cifar_x: x_tmp, cifar_y: y_tmp})\n",
    "# train the model \n",
    "for i in range(5):\n",
    "    x_batch, _ = my_generic_batch((x_train, y_train), n = 2)\n",
    "    x_batch = x_batch.reshape((x_batch.shape[0], 32 * 32 * 3))\n",
    "    sess.run(cifar_optim_step, feed_dict = {cifar_x: x_batch, cifar_y: x_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(optimizer, loss_func, model_logits = logits, dataset = \"mnist\", n_epochs = 1000, reshape = True):\n",
    "    # train the model a bit\n",
    "    for i in range(1000):\n",
    "        if dataset == \"mnist\":\n",
    "            x_batch, y_batch = mnist.train.next_batch(100)\n",
    "        else:\n",
    "            x_batch, y_batch = my_generic_batch(dataset, n = 10)\n",
    "            if reshape:\n",
    "                print(x_batch.shape)\n",
    "                x_batch = x_batch.reshape((x_batch.shape[0], 32 * 32 * 3))\n",
    "        optimizer.run(feed_dict = {cifar_x: x_batch, cifar_y: x_batch})\n",
    "        if i % 50 == 0:\n",
    "            cur_loss = loss_func.eval(feed_dict = {cifar_x: x_batch, cifar_y: x_batch})\n",
    "            print(cur_loss)\n",
    "            # sample an image from the model and plot it\n",
    "            decoded_images = sess.run(model_logits, feed_dict = {cifar_x: x_batch[:5]})\n",
    "            if dataset == \"mnist\":\n",
    "                plot(decoded_images)\n",
    "            else:\n",
    "                plot(decoded_images, shape = (32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NOTE: don't run this with the conv model.\n",
    "train_model(optimizer = cifar_optim_step, loss_func = cifar_training_loss, model_logits = cifar_logits, dataset = (x_train, y_train), n_epochs = 1000, reshape = True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
